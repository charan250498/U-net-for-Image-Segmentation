{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S7DHV3pqERC"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XIJsXYmUp8qO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import random\n",
    "\n",
    "#import any other library you need below this line\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOBtE8PqH4w"
   },
   "source": [
    "### Loading data\n",
    "\n",
    "Upload the data in zip format to Colab. Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VMLW_lgTqRcL"
   },
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UoM-TMIqTna"
   },
   "source": [
    "### Defining the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6awOO200qYSZ"
   },
   "outputs": [],
   "source": [
    "class Cell_data(Dataset):\n",
    "    def __init__(self, data_dir, size, train = True, train_test_split = 0.8, augment_data = True, transform = None):\n",
    "        ##########################inputs##################################\n",
    "        #data_dir(string) - directory of the data#########################\n",
    "        #size(int) - size of the images you want to use###################\n",
    "        #train(boolean) - train data or test data#########################\n",
    "        #train_test_split(float) - the portion of the data for training###\n",
    "        #augment_data(boolean) - use data augmentation or not#############\n",
    "        super(Cell_data, self).__init__()\n",
    "        # todo\n",
    "        #initialize the data class\n",
    "        self.image_files = os.listdir(data_dir+\"/scans\")\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        train_test_split = math.floor(train_test_split*len(self.image_files))\n",
    "\n",
    "        self.augmented_images_dir = data_dir+\"/augmented_data\"\n",
    "        if not os.path.exists(self.augmented_images_dir):\n",
    "            os.mkdir(self.augmented_images_dir)\n",
    "\n",
    "        random.shuffle(self.image_files)\n",
    "        if train == True:\n",
    "            train_image_files = self.augmented_images_dir+\"/train\"\n",
    "            os.mkdir(train_image_files)\n",
    "            os.mkdir(train_image_files+\"/scans\")\n",
    "            os.mkdir(train_image_files+\"/labels\")\n",
    "            self.final_images_dir = train_image_files\n",
    "            dataset_for_augmentation = self.image_files[:train_test_split]\n",
    "        else:\n",
    "            test_image_files = self.augmented_images_dir+\"/test\"\n",
    "            os.mkdir(test_image_files)\n",
    "            os.mkdir(test_image_files+\"/scans\")\n",
    "            os.mkdir(test_image_files+\"/labels\")\n",
    "            self.final_images_dir = test_image_files\n",
    "            dataset_for_augmentation = self.image_files[train_test_split:]\n",
    "\n",
    "        image_count=1\n",
    "        for image in dataset_for_augmentation:\n",
    "            scan_image = Image.open(data_dir+\"/scans/\"+image)\n",
    "            label_image = Image.open(data_dir+\"/labels/\"+image)\n",
    "            #data augmentation part\n",
    "            if augment_data:\n",
    "                augment_mode = np.random.randint(0, 3) ############## Ignoring the case of zoom and gamma correction so reduced range from 5 to 3\n",
    "                if augment_mode == 0:\n",
    "                    #todo \n",
    "                    #flip image vertically\n",
    "                    augmented_scan_image = TF.vflip(scan_image)\n",
    "                    augmented_label_image = TF.vflip(label_image)\n",
    "                elif augment_mode == 1:\n",
    "                    #todo\n",
    "                    #flip image horizontally\n",
    "                    augmented_scan_image = TF.hflip(scan_image)\n",
    "                    augmented_label_image = TF.hflip(label_image)\n",
    "                elif augment_mode == 2:\n",
    "                    #todo\n",
    "                    #rotate image\n",
    "                    angle = random.randint(-30, 30)\n",
    "                    augmented_scan_image = TF.rotate(scan_image, angle)\n",
    "                    augmented_label_image = TF.rotate(label_image, angle)\n",
    "                elif augment_mode == 3: # Not used for now\n",
    "                    #Gamma correction\n",
    "                    gamma = random.randrange(0,3)/2\n",
    "                    augmented_scan_image = TF.adjust_gamma(scan_image, gamma)\n",
    "                    augmented_label_image = label_image\n",
    "                else: # Not used for now\n",
    "                    #todo\n",
    "                    #zoom image\n",
    "                    zoom_size = random.randint(int(self.size*0.75), self.size)\n",
    "                    augmented_scan_image = TF.center_crop(scan_image, zoom_size)\n",
    "                    augmented_label_image = TF.center_crop(label_image, zoom_size)\n",
    "                \n",
    "                #write the files to the augmented data location\n",
    "                scan_image.save(self.final_images_dir+\"/scans/\"+str(image_count)+\".bmp\") # original image is saved as 1.bmp\n",
    "                augmented_scan_image.save(self.final_images_dir+\"/scans/\"+str(image_count+1)+\".bmp\") # augmented image is saved as 2.jpg\n",
    "                label_image.save(self.final_images_dir+\"/labels/\"+str(image_count)+\".bmp\") # original label is saved as 1.bmp\n",
    "                augmented_label_image.save(self.final_images_dir+\"/labels/\"+str(image_count+1)+\".bmp\") # augmented label is saved as 2.jpg\n",
    "                image_count+=2\n",
    "        \n",
    "        self.dataset = os.listdir(self.final_images_dir+\"/scans\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # todo\n",
    "        #load image and mask from index idx of your data\n",
    "        filename = self.dataset[idx]\n",
    "        scan_image = Image.open(self.final_images_dir+\"/scans/\"+filename)\n",
    "        label_image = Image.open(self.final_images_dir+\"/labels/\"+filename)\n",
    "        #todo\n",
    "        #return image and mask in tensors\n",
    "        return self.transform(scan_image), self.transform(label_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo6kRDASsc5t"
   },
   "source": [
    "### Define the Model\n",
    "1. Define the Convolution blocks\n",
    "2. Define the down path\n",
    "3. Define the up path\n",
    "4. combine the down and up path to get the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qcOEN68psaxF"
   },
   "outputs": [],
   "source": [
    "class twoConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(twoConvBlock, self).__init__()\n",
    "        #todo\n",
    "        #initialize the block\n",
    "        self.conv_layer1 = nn.Conv2d(input_channels, output_channels, kernel_size=5, stride=1)\n",
    "        self.conv_layer2 = nn.Conv2d(output_channels, output_channels, kernel_size=5, stride=1)\n",
    "        self.batch_norm_layer = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.conv_layer1(image)\n",
    "        image = F.relu(image)\n",
    "        image = self.conv_layer2(image)\n",
    "        image = self.batch_norm_layer(image)\n",
    "        image = F.relu(image)\n",
    "        return image\n",
    "\n",
    "class downStep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downStep, self).__init__()\n",
    "        #todo\n",
    "        #initialize the down path\n",
    "        self.max_pool_layer = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.max_pool_layer(image)\n",
    "        return image\n",
    "\n",
    "class upStep(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(upStep, self).__init__()\n",
    "        #todo\n",
    "        #initialize the up path\n",
    "        self.up_sampling_layer = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.up_sampling_layer(image)\n",
    "        return image\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        #todo\n",
    "        #initialize the complete model\n",
    "        self.conv1 = twoConvBlock(1, 64)\n",
    "        self.conv2 = twoConvBlock(63, 128)\n",
    "        self.conv3 = twoConvBlock(128, 256)\n",
    "        self.conv4 = twoConvBlock(256, 512)\n",
    "        self.conv5 = twoConvBlock(512, 1024)\n",
    "        self.conv6 = twoConvBlock(1024, 512)\n",
    "        self.conv7 = twoConvBlock(512, 256)\n",
    "        self.conv8 = twoConvBlock(256, 128)\n",
    "        self.conv9 = twoConvBlock(128, 64)\n",
    "        self.conv10 = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "        self.down_step = downStep()\n",
    "        self.up_step1 = upStep(1024, 512)\n",
    "        self.up_step2 = upStep(512, 256)\n",
    "        self.up_step3 = upStep(256,128)\n",
    "        self.up_step4 = upStep(128,64)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.down_step(self.conv1(image))\n",
    "        image = self.down_step(self.conv2(image))\n",
    "        image = self.down_step(self.conv3(image))\n",
    "        image = self.down_step(self.conv4(image))\n",
    "        image = self.up_step1(self.conv5(image)) # we need to concat some stuff here after.\n",
    "        image = self.up_step2(self.conv6(image))\n",
    "        image = self.up_step3(self.conv7(image))\n",
    "        image = self.up_step4(self.conv8(image))\n",
    "        image = self.conv9(image)\n",
    "        image = self.conv10(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5-0LnQItdth"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NmFg17HktfBW"
   },
   "outputs": [],
   "source": [
    "#Paramteres\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "#number of training epochs\n",
    "epoch_n = 2\n",
    "\n",
    "#input image-mask size\n",
    "image_size = 572\n",
    "#root directory of project\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "#training batch size\n",
    "batch_size = 4\n",
    "\n",
    "#use checkpoint model for training\n",
    "load = False\n",
    "\n",
    "#use GPU for training\n",
    "gpu = True\n",
    "\n",
    "data_dir = os.path.join(root_dir, 'data/cells')\n",
    "\n",
    "\n",
    "trainset = Cell_data(data_dir = data_dir, size = image_size, transform = transform)\n",
    "trainloader = DataLoader(trainset, batch_size = 4, shuffle=True)\n",
    "\n",
    "testset = Cell_data(data_dir = data_dir, size = image_size, train = False, transform = transform)\n",
    "testloader = DataLoader(testset, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0' if gpu else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = UNet().to('cuda:0').to(device)\n",
    "model = UNet().to(device)\n",
    "\n",
    "if load:\n",
    "    print('loading model')\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#, lr=lr, momentum=0.99, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([4, 1, 1024, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 63, 5, 5], expected input[4, 64, 508, 508] to have 63 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m pred \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m     15\u001b[0m crop_x \u001b[39m=\u001b[39m (label\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m pred\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     16\u001b[0m crop_y \u001b[39m=\u001b[39m (label\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m pred\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [4], line 71\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m     68\u001b[0m     \u001b[39m#todo\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#implement the forward path\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(image))\n\u001b[1;32m---> 71\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(image))\n\u001b[0;32m     72\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(image))\n\u001b[0;32m     73\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(image))\n",
      "File \u001b[1;32mc:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [4], line 13\u001b[0m, in \u001b[0;36mtwoConvBlock.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m     11\u001b[0m     \u001b[39m#todo\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m#implement the forward path\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layer1(image)\n\u001b[0;32m     14\u001b[0m     image \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(image)\n\u001b[0;32m     15\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layer2(image)\n",
      "File \u001b[1;32mc:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 63, 5, 5], expected input[4, 64, 508, 508] to have 63 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(epoch_n):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        image, label = data\n",
    "        print(image.device)\n",
    "        print(label.shape)\n",
    "\n",
    "        image = image.unsqueeze(1).to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "\n",
    "        crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "        crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "        label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "    \n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n",
    "    print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss / trainset.__len__()))\n",
    "\n",
    "    torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            image, label = data\n",
    "\n",
    "            image = image.unsqueeze(1).to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            pred = model(image)\n",
    "            crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "            crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "            label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_labels = torch.max(pred, dim = 1)\n",
    "\n",
    "            total += label.shape[0] * label.shape[1] * label.shape[2]\n",
    "            correct += (pred_labels == label).sum().item()\n",
    "\n",
    "        print('Accuracy: %.4f ---- Loss: %.4f' % (correct / total, total_loss / testset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-64s70tyBw"
   },
   "source": [
    "### Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko9zFomNuCfC"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "output_masks = []\n",
    "output_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(testset.__len__()):\n",
    "        image, labels = testset.__getitem__(i)\n",
    "    \n",
    "        input_image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        pred = model(input_image)\n",
    "\n",
    "        output_mask = torch.max(pred, dim = 1)[1].cpu().squeeze(0).numpy()\n",
    "\n",
    "        crop_x = (labels.shape[0] - output_mask.shape[0]) // 2\n",
    "        crop_y = (labels.shape[1] - output_mask.shape[1]) // 2\n",
    "        labels = labels[crop_x: labels.shape[0] - crop_x, crop_y: labels.shape[1] - crop_y].numpy()\n",
    "    \n",
    "        output_masks.append(output_mask)\n",
    "        output_labels.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OrV7k1GuFSA"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(testset.__len__(), 2, figsize = (20, 20))\n",
    "\n",
    "for i in range(testset.__len__()):\n",
    "    axes[i, 0].imshow(output_labels[i])\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(output_masks[i])\n",
    "    axes[i, 1].axis('off')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UNet_FW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2621737be63ac2724b36668addeeaabe549ad162002460e4cf3d1830cb453f3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
