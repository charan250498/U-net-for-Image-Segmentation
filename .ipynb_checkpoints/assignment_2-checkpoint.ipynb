{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S7DHV3pqERC"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XIJsXYmUp8qO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import random\n",
    "\n",
    "#import any other library you need below this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOBtE8PqH4w"
   },
   "source": [
    "### Loading data\n",
    "\n",
    "Upload the data in zip format to Colab. Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VMLW_lgTqRcL"
   },
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UoM-TMIqTna"
   },
   "source": [
    "### Defining the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6awOO200qYSZ"
   },
   "outputs": [],
   "source": [
    "class Cell_data(Dataset):\n",
    "    def __init__(self, data_dir, size, train = 'True', train_test_split = 0.8, augment_data = True):\n",
    "        ##########################inputs##################################\n",
    "        #data_dir(string) - directory of the data#########################\n",
    "        #size(int) - size of the images you want to use###################\n",
    "        #train(boolean) - train data or test data#########################\n",
    "        #train_test_split(float) - the portion of the data for training###\n",
    "        #augment_data(boolean) - use data augmentation or not#############\n",
    "        super(Cell_data, self).__init__()\n",
    "        # todo\n",
    "        #initialize the data class\n",
    "        self.scan_dir = data_dir+\"/scans/\"\n",
    "        self.label_dir = data_dir+\"/labels/\"\n",
    "        self.image_files = os.listdir(self.scan_dir)\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "        self.train_test_split = train_test_split\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        for image in self.image_files:\n",
    "            scan_image = Image.open(self.scan_dir+image)\n",
    "            label_image = Image.open(self.label_dir+image)\n",
    "            #data augmentation part\n",
    "            if augment_data:\n",
    "                augment_mode = np.random.randint(0, 4)\n",
    "                if augment_mode == 0:\n",
    "                    #todo \n",
    "                    #flip image vertically\n",
    "                    augmented_scan_image = TF.vflip(scan_image)\n",
    "                    augmented_label_image = TF.vflip(label_image)\n",
    "                elif augment_mode == 1:\n",
    "                    #todo\n",
    "                    #flip image horizontally\n",
    "                    augmented_scan_image = TF.hflip(scan_image)\n",
    "                    augmented_label_image = TF.hflip(label_image)\n",
    "                elif augment_mode == 2:\n",
    "                    #todo\n",
    "                    #zoom image\n",
    "                    zoom_size = random.randint(self.size*0.75, self.size)\n",
    "                    augmented_scan_image = TF.center_crop(scan_image, zoom_size)\n",
    "                    augmented_label_image = TF.center_crop(label_image, zoom_size)\n",
    "                else:\n",
    "                    #todo\n",
    "                    #rotate image\n",
    "                    angle = random.randint(-30, 30)\n",
    "                    augmented_scan_image = TF.rotate(scan_image, angle)\n",
    "                    augmented_label_image = TF.rotate(label_image, angle)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # todo\n",
    "        #load image and mask from index idx of your data\n",
    "        filename = self.image_files[idx]\n",
    "\n",
    "        #todo\n",
    "        #return image and mask in tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo6kRDASsc5t"
   },
   "source": [
    "### Define the Model\n",
    "1. Define the Convolution blocks\n",
    "2. Define the down path\n",
    "3. Define the up path\n",
    "4. combine the down and up path to get the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qcOEN68psaxF"
   },
   "outputs": [],
   "source": [
    "class twoConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(twoConvBlock, self).__init__()\n",
    "        #todo\n",
    "        #initialize the block\n",
    "        self.conv_layer1 = nn.Conv2d(input_channels, output_channels, kernel_size=5, stride=1)\n",
    "        self.conv_layer2 = nn.Conv2d(output_channels, output_channels, kernel_size=5, stride=1)\n",
    "        self.batch_norm_layer = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.conv_layer1(image)\n",
    "        image = F.relu(image)\n",
    "        image = self.conv_layer2(image)\n",
    "        image = self.batch_norm_layer(image)\n",
    "        image = F.relu(image)\n",
    "        return image\n",
    "\n",
    "class downStep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downStep, self).__init__()\n",
    "        #todo\n",
    "        #initialize the down path\n",
    "        self.max_pool_layer = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.max_pool_layer(image)\n",
    "        return image\n",
    "\n",
    "class upStep(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(upStep, self).__init__()\n",
    "        #todo\n",
    "        #initialize the up path\n",
    "        self.up_sampling_layer = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.up_sampling_layer(image)\n",
    "        return image\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        #todo\n",
    "        #initialize the complete model\n",
    "        self.conv1 = twoConvBlock(1, 64)\n",
    "        self.conv2 = twoConvBlock(63, 128)\n",
    "        self.conv3 = twoConvBlock(128, 256)\n",
    "        self.conv4 = twoConvBlock(256, 512)\n",
    "        self.conv5 = twoConvBlock(512, 1024)\n",
    "        self.conv6 = twoConvBlock(1024, 512)\n",
    "        self.conv7 = twoConvBlock(512, 256)\n",
    "        self.conv8 = twoConvBlock(256, 128)\n",
    "        self.conv9 = twoConvBlock(128, 64)\n",
    "        self.conv10 = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "        self.down_step = downStep()\n",
    "        self.up_step = upStep()\n",
    "\n",
    "    def forward(self, image):\n",
    "        #todo\n",
    "        #implement the forward path\n",
    "        image = self.down_step(self.conv1(image))\n",
    "        image = self.down_step(self.conv2(image))\n",
    "        image = self.down_step(self.conv3(image))\n",
    "        image = self.down_step(self.conv4(image))\n",
    "        image = self.up_step(self.conv5(image))\n",
    "        image = self.up_step(self.conv6(iamge))\n",
    "        image = self.up_step(self.conv7(image))\n",
    "        image = self.up_step(self.conv8(image))\n",
    "        image = self.up_step(self.conv9(image))\n",
    "        image = self.conv10(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5-0LnQItdth"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NmFg17HktfBW"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cell_data' object has no attribute 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cells\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m trainset \u001b[38;5;241m=\u001b[39m Cell_data(data_dir \u001b[38;5;241m=\u001b[39m data_dir, size \u001b[38;5;241m=\u001b[39m image_size)\n\u001b[1;32m---> 27\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(trainset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m testset \u001b[38;5;241m=\u001b[39m Cell_data(data_dir \u001b[38;5;241m=\u001b[39m data_dir, size \u001b[38;5;241m=\u001b[39m image_size, train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m testloader \u001b[38;5;241m=\u001b[39m DataLoader(testset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:353\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "Cell \u001b[1;32mIn [5], line 58\u001b[0m, in \u001b[0;36mCell_data.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Cell_data' object has no attribute 'images'"
     ]
    }
   ],
   "source": [
    "#Paramteres\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "#number of training epochs\n",
    "epoch_n = 20\n",
    "\n",
    "#input image-mask size\n",
    "image_size = 572\n",
    "#root directory of project\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "#training batch size\n",
    "batch_size = 4\n",
    "\n",
    "#use checkpoint model for training\n",
    "load = False\n",
    "\n",
    "#use GPU for training\n",
    "gpu = True\n",
    "\n",
    "data_dir = os.path.join(root_dir, 'data/cells')\n",
    "\n",
    "\n",
    "trainset = Cell_data(data_dir = data_dir, size = image_size)\n",
    "trainloader = DataLoader(trainset, batch_size = 4, shuffle=True)\n",
    "\n",
    "testset = Cell_data(data_dir = data_dir, size = image_size, train = False)\n",
    "testloader = DataLoader(testset, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if gpu else 'cpu')\n",
    "\n",
    "model = UNet().to('cuda:0').to(device)\n",
    "\n",
    "if load:\n",
    "    print('loading model')\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, momentum=0.99, weight_decay=0.0005)\n",
    "\n",
    "model.train()\n",
    "for e in range(epoch_n):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        image, label = data\n",
    "\n",
    "        image = image.unsqueeze(1).to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "\n",
    "        crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "        crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "        label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "    \n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n",
    "    print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss / trainset.__len__()))\n",
    "\n",
    "    torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            image, label = data\n",
    "\n",
    "            image = image.unsqueeze(1).to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            pred = model(image)\n",
    "            crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "            crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "            label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_labels = torch.max(pred, dim = 1)\n",
    "\n",
    "            total += label.shape[0] * label.shape[1] * label.shape[2]\n",
    "            correct += (pred_labels == label).sum().item()\n",
    "\n",
    "        print('Accuracy: %.4f ---- Loss: %.4f' % (correct / total, total_loss / testset.__len__()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-64s70tyBw"
   },
   "source": [
    "### Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko9zFomNuCfC"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "output_masks = []\n",
    "output_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(testset.__len__()):\n",
    "        image, labels = testset.__getitem__(i)\n",
    "    \n",
    "        input_image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        pred = model(input_image)\n",
    "\n",
    "        output_mask = torch.max(pred, dim = 1)[1].cpu().squeeze(0).numpy()\n",
    "\n",
    "        crop_x = (labels.shape[0] - output_mask.shape[0]) // 2\n",
    "        crop_y = (labels.shape[1] - output_mask.shape[1]) // 2\n",
    "        labels = labels[crop_x: labels.shape[0] - crop_x, crop_y: labels.shape[1] - crop_y].numpy()\n",
    "    \n",
    "        output_masks.append(output_mask)\n",
    "        output_labels.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OrV7k1GuFSA"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(testset.__len__(), 2, figsize = (20, 20))\n",
    "\n",
    "for i in range(testset.__len__()):\n",
    "    axes[i, 0].imshow(output_labels[i])\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(output_masks[i])\n",
    "    axes[i, 1].axis('off')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UNet_FW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2621737be63ac2724b36668addeeaabe549ad162002460e4cf3d1830cb453f3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
